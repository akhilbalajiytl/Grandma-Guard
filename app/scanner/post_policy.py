# app/scanner/post_policy.py

# Define the high-confidence failure thresholds for our Garak detectors.
# A score >= this value from the corresponding detector will trigger a block.
BLOCKING_THRESHOLDS = {
    "refusal_v2": 0.95,  # If the model clearly complies with a harmful request.
    "xss_markdown_basic": 0.95,  # If it generates a basic XSS payload.
    "python_code_execution": 0.95,  # If it generates malicious-looking code.
    "sqli_echo": 0.95,  # If it echoes a SQL injection payload.
    "win_prodkey_5x5": 0.95,  # If it generates a Windows product key.
    "toxicity_comment_model": 0.85,  # Be a bit more strict on toxicity.
    "slur_detection": 0.95,  # If it generates a slur.
}


def PostGenerationPolicyEngine(risk_profile: dict) -> (str, str):
    """
    Analyzes the Garak forensic risk profile AFTER a response has been generated.
    Decides whether the response is safe enough to send to the user.

    Returns:
        A tuple of (decision, reason_string)
        decision: "BLOCK" or "ALLOW"
        reason: The rule that was triggered.
    """
    if not risk_profile or "scores" not in risk_profile:
        return "ALLOW", "No risk profile available."

    scores = risk_profile.get("scores", {})

    for detector_name, threshold in BLOCKING_THRESHOLDS.items():
        score = scores.get(detector_name, 0.0)
        if score >= threshold:
            reason = f"Post-Gen Block: Detector '{detector_name}' score ({score:.2f}) exceeded threshold ({threshold})."
            print(f"ðŸš¨ {reason}")
            return "BLOCK", reason

    return "ALLOW", "All post-generation checks passed."
